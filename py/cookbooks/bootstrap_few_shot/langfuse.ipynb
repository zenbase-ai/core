{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import the Zenbase Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {package}: {e}\")\n",
    "        raise\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        install_package(package)\n",
    "\n",
    "try:\n",
    "    # Check if running in Google Colab\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install the zenbase package if running in Google Colab\n",
    "    # install_package('zenbase')\n",
    "    # Install the zenbse package from a GitHub branch if running in Google Colab\n",
    "    install_package('git+https://github.com/zenbase-ai/lib.git@main#egg=zenbase&subdirectory=py')\n",
    "\n",
    "    # List of other packages to install in Google Colab\n",
    "    additional_packages = [\n",
    "        'python-dotenv',\n",
    "        'langfuse',\n",
    "        'openai',\n",
    "        'langchain',\n",
    "        'langchain_openai'\n",
    "    ]\n",
    "    \n",
    "    # Install additional packages\n",
    "    install_packages(additional_packages)\n",
    "\n",
    "# Now import the zenbase library\n",
    "try:\n",
    "    import zenbase\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import zenbase: \", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "#\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"...\"\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\"\n",
    "\n",
    "load_dotenv(Path(\"../../.env.test\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()\n",
    "langfuse.auth_check()\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Now, you probably already have some LLM code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It could use the OpenAI SDK, LangChain, or anything really. But it looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@observe()\n",
    "def solver(inputs):\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Solve the given problem using the provided plan and operations.\n",
    "        Return only the final numerical answer, without any additional text or explanation.\"\"\"),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Mathematical Operation: {operation}\"),\n",
    "        (\"user\", \"Provide the final numerical answer:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "\n",
    "    plan = planner_chain(inputs)\n",
    "    operation = operation_finder({\"plan\": plan[\"plan\"], \"question\": inputs[\"question\"]})\n",
    "    \n",
    "    inputs_to_answer = {\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"operation\": operation[\"operation\"],\n",
    "    }\n",
    "    answer = chain.invoke(inputs_to_answer)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "@observe()\n",
    "def planner_chain(inputs):\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Create a step-by-step plan to solve the given problem.\n",
    "        Be clear and concise in your steps.\"\"\"),\n",
    "        (\"user\", \"Problem: {question}\\n\\nProvide a step-by-step plan to solve this problem:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    plan = chain.invoke(inputs)\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "@observe()\n",
    "def operation_finder(inputs):\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Identify the overall mathematical operation needed to solve the problem \n",
    "        based on the given plan. Use simple operations like addition, subtraction, multiplication, and division.\"\"\"),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Identify the primary mathematical operation needed:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    operation = chain.invoke(inputs)\n",
    "    return {\"operation\": operation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## And let's say you have an eval function like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_answer(answer: str, expected_output: dict):\n",
    "    \"\"\"The first argument is the return value from the `langchain_chain` function above.\"\"\"\n",
    "    score = int(answer['answer'] == expected_output.split(\"#### \")[-1])\n",
    "    langfuse.score(\n",
    "        name=\"correctness\",\n",
    "        value=score,\n",
    "        trace_id=langfuse.get_trace_id(),\n",
    "    )\n",
    "    return {\"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Then you're probably evaluating like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalset = langfuse.get_dataset(\"gsm8k-testset\")\n",
    "\n",
    "scores = []\n",
    "for item in evalset.items:\n",
    "    answer = solver(item.input)\n",
    "    eval = score_answer(answer, item.expected_output)\n",
    "    scores.append(eval[\"score\"])\n",
    "\n",
    "print(\"Average score\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Now, how can we optimize this score of 0.6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## First, initialize the Zenbase ZenbaseTracer and import the Langfuse helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.adaptors.langfuse_helper import ZenLangfuse\n",
    "from zenbase.core.managers import ZenbaseTracer\n",
    "\n",
    "zen_langfuse_adaptor = ZenLangfuse(langfuse)\n",
    "zenbase_tracer = ZenbaseTracer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hook up Zenbase to your functions\n",
    "\n",
    "1. Use the `zenbase_tracer` decorator.\n",
    "2. Change function inputs to request\n",
    "3. Use request's `zenbase.task_demos` to get the few-shot examples for the task and add them however you would like into your prompt.\n",
    "4. If you need to use just a few examples, you can use `request.zenbase.task_demos[:2]` to get the first two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenbase.types import LMRequest, LMDemo\n",
    "\n",
    "@zenbase_tracer # it is 1\n",
    "@observe()\n",
    "def solver(request: LMRequest): # it is 2\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Solve the given problem using the provided plan and operations.\n",
    "        Return only the final numerical answer, without any additional text or explanation.\"\"\")\n",
    "    ]\n",
    "    \n",
    "    for demo in request.zenbase.task_demos: # it is 3\n",
    "        the_output = demo.outputs[\"answer\"] if isinstance(demo.outputs, dict) else demo.outputs\n",
    "        messages += [\n",
    "            (\"user\", f'Example Question: {demo.inputs[\"question\"]}'),\n",
    "            (\"assistant\", f\"Example Answer: {the_output}\"),\n",
    "        ]\n",
    "\n",
    "    messages += [\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Mathematical Operation: {operation}\"),\n",
    "        (\"user\", \"Provide the final numerical answer:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "\n",
    "    plan = planner_chain(request.inputs)\n",
    "    operation = operation_finder({\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"question\": request.inputs[\"question\"],\n",
    "    })\n",
    "    \n",
    "    inputs_to_answer = {\n",
    "        \"question\": request.inputs[\"question\"],\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"operation\": operation[\"operation\"],\n",
    "    }\n",
    "    answer = chain.invoke(inputs_to_answer)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "@zenbase_tracer # it is 1\n",
    "@observe()\n",
    "def planner_chain(request: LMRequest): # it is 2\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Create a step-by-step plan to solve the given problem.\n",
    "        Be clear and concise in your steps.\"\"\")\n",
    "    ]\n",
    "    \n",
    "    if request.zenbase.task_demos: # it is 3\n",
    "        for demo in request.zenbase.task_demos[:2]: # it is 4\n",
    "            messages += [\n",
    "                (\"user\", demo.inputs[\"question\"]),\n",
    "                (\"assistant\", demo.outputs[\"plan\"]),\n",
    "            ]\n",
    "\n",
    "    messages.append((\"user\", \"Problem: {question}\\n\\nProvide a step-by-step plan to solve this problem:\"))\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    plan = chain.invoke(request.inputs)\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "@zenbase_tracer # it is 1\n",
    "@observe()\n",
    "def operation_finder(request: LMRequest): # it is 2\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Identify the overall mathematical operation needed to solve the problem \n",
    "        based on the given plan. Use simple operations like addition, subtraction, multiplication, and division.\"\"\")\n",
    "    ]\n",
    "    \n",
    "    if request.zenbase.task_demos: # it is 3\n",
    "        for demo in request.zenbase.task_demos[:2]: # it is 4\n",
    "            messages += [\n",
    "                (\"user\", demo.inputs[\"question\"]),\n",
    "                (\"user\", demo.inputs[\"plan\"]),\n",
    "                (\"assistant\", demo.outputs[\"operation\"]),\n",
    "            ]\n",
    "\n",
    "    messages += [\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Identify the primary mathematical operation needed:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    operation = chain.invoke(request.inputs)\n",
    "    return {\"operation\": operation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## For Langfuse, we have to update our eval function a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_answer_with_json(answer: str, demo: LMDemo, langfuse: Langfuse):\n",
    "    \"\"\"The first argument is the return value from the `langchain_chain` function above.\"\"\"\n",
    "    score = int(answer[\"answer\"] == demo.outputs.split(\"#### \")[-1])\n",
    "    langfuse.score(\n",
    "        name=\"correctness\",\n",
    "        value=score,\n",
    "        trace_id=langfuse.get_trace_id(),\n",
    "    )\n",
    "    return {\"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now we can optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Set up your optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.optim.metric.bootstrap_few_shot import BootstrapFewShot\n",
    "\n",
    "train_set = 'GSM8K_train_set_langsmith_dataset_2it3BpoNmwfYa5Nvk6dRButWA56'\n",
    "validation_set = 'GSM8K_validation_set_langsmith_dataset_2it1pPsf4w75FJ82v5BwweOxnS4'\n",
    "test_set = 'GSM8K_test_set_langsmith_dataset_2it1pZxHYNfqO8wHTsRP7NiUi1e'\n",
    "SHOTS = 2\n",
    "SAMPLES = 2\n",
    "\n",
    "\n",
    "evaluator_kwargs = dict(\n",
    "    evaluate=score_answer_with_json,\n",
    ")\n",
    "\n",
    "\n",
    "bootstrap_few_shot = BootstrapFewShot(\n",
    "    shots=SHOTS,\n",
    "    training_set=train_set,\n",
    "    test_set=test_set,\n",
    "    validation_set=validation_set,\n",
    "    evaluator_kwargs=evaluator_kwargs,\n",
    "    zen_adaptor=zen_langfuse_adaptor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Do the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Empty the traces\n",
    "zenbase_tracer.all_traces = {}\n",
    "# Run the optimization\n",
    "best_fn, candidates = bootstrap_few_shot.perform(\n",
    "    solver,\n",
    "    samples=SAMPLES,\n",
    "    rounds=1,\n",
    "    trace_manager=zenbase_tracer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use your optimized function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zenbase_tracer.all_traces = {}\n",
    "best_fn({\"question\": \"What is 2 + 2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Introspect function traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function_traces = [v for k, v in zenbase_tracer.all_traces.items()][0][\"optimized\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for planner_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"planner_chain\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for operation_finder chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"operation_finder\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"solver\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## How to save the function and load it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save the optimized function args to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.save_optimizer_args(\"bootstrap_few_shot_args.zenbase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the optimized function args with the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.save_optimizer_args(\"bootstrap_few_shot_args.zenbase\")\n",
    "\n",
    "optimized_function = bootstrap_few_shot.load_optimizer_and_function(\"bootstrap_few_shot_args.zenbase\", solver, zenbase_tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use the loaded function and make sure it loaded the demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zenbase_tracer.all_traces = {}\n",
    "optimized_function({\"question\": \"If I have 30% of shares, and Mo has 24.5% of shares, how many of our 10M shares are unassigned?\"})\n",
    "function_traces = [v for k, v in zenbase_tracer.all_traces.items()][0][\"optimized\"]\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"solver\"][\"args\"][\"request\"].zenbase.task_demos)\n",
    "pprint(function_traces[\"planner_chain\"][\"args\"][\"request\"].zenbase.task_demos)\n",
    "pprint(function_traces[\"operation_finder\"][\"args\"][\"request\"].zenbase.task_demos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
