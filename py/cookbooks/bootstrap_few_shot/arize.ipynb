{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import the Zenbase Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {package}: {e}\")\n",
    "        raise\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        install_package(package)\n",
    "\n",
    "try:\n",
    "    # Check if running in Google Colab\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install the zenbase package if running in Google Colab\n",
    "    # install_package('zenbase')\n",
    "    # Install the zenbse package from a GitHub branch if running in Google Colab\n",
    "    install_package('git+https://github.com/zenbase-ai/lib.git@main#egg=zenbase&subdirectory=py')\n",
    "\n",
    "    # List of other packages to install in Google Colab\n",
    "    additional_packages = [\n",
    "        'python-dotenv',\n",
    "        'arize-phoenix[evals]',\n",
    "        'openai',\n",
    "        'langchain',\n",
    "        'langchain_openai'\n",
    "    ]\n",
    "    \n",
    "    # Install additional packages\n",
    "    install_packages(additional_packages)\n",
    "\n",
    "# Now import the zenbase library\n",
    "try:\n",
    "    import zenbase\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import zenbase: \", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "#\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "\n",
    "load_dotenv(Path(\"../../.env.test\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the phoenix app\n",
    "import phoenix as px\n",
    "px.launch_app()\n",
    "# initiate the phoenix client\n",
    "arize_phoenix = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.utils import ksuid\n",
    "from zenbase.adaptors.arize import ZenArizeAdaptor\n",
    "zen_arize_adaptor = ZenArizeAdaptor(arize_phoenix)\n",
    "\n",
    "# setup datasets\n",
    "import datasets\n",
    "gsm8k_dataset = datasets.load_dataset(\"gsm8k\", \"main\")\n",
    "TESTSET_SIZE = 2\n",
    "TRAINSET_SIZE = 5\n",
    "VALIDATIONSET_SIZE = 2\n",
    "\n",
    "\n",
    "def create_dataset_with_examples(zen_arize_adaptor: ZenArizeAdaptor, prefix: str, item_set: list) -> str:\n",
    "    dataset_name = ksuid(prefix=prefix)\n",
    "\n",
    "    inputs = [{\"question\": example[\"question\"]} for example in item_set]\n",
    "    expected_outputs = [{\"answer\": example[\"answer\"]} for example in item_set]\n",
    "    zen_arize_adaptor.add_examples_to_dataset(dataset_name, inputs, expected_outputs)\n",
    "    return dataset_name\n",
    "\n",
    "train_set = create_dataset_with_examples(\n",
    "        zen_arize_adaptor,\n",
    "        \"GSM8K_train_set\",\n",
    "        list(gsm8k_dataset[\"train\"].select(range(TRAINSET_SIZE))),\n",
    "    )\n",
    "\n",
    "validation_set = create_dataset_with_examples(\n",
    "        zen_arize_adaptor,\n",
    "        \"GSM8K_validation_set\",\n",
    "        list(gsm8k_dataset[\"train\"].select(range(TRAINSET_SIZE + 1, TRAINSET_SIZE + VALIDATIONSET_SIZE + 1))),\n",
    "    )\n",
    "\n",
    "test_set = create_dataset_with_examples(\n",
    "        zen_arize_adaptor,\n",
    "        \"GSM8K_test_set\",\n",
    "        list(gsm8k_dataset[\"test\"].select(range(TESTSET_SIZE))),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Now, you probably already have some LLM code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It could use the OpenAI SDK, LangChain, or anything really. But it looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def solver(inputs):\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert math solver. Solve the given problem using the provided plan and operations.\n",
    "        Return only the final numerical answer, without any additional text or explanation.\"\"\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    messages.extend(\n",
    "        [\n",
    "            (\"user\", \"Question: {question}\"),\n",
    "            (\"user\", \"Plan: {plan}\"),\n",
    "            (\"user\", \"Mathematical Operation: {operation}\"),\n",
    "            (\"user\", \"Provide the final numerical answer:\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "\n",
    "    plan = planner_chain(inputs)\n",
    "    operation = operation_finder({\"plan\": plan[\"plan\"], \"question\": inputs[\"question\"]})\n",
    "\n",
    "    inputs_to_answer = {\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"operation\": operation[\"operation\"],\n",
    "    }\n",
    "    answer = chain.invoke(inputs_to_answer)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "def planner_chain(inputs):\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert math solver. Create a step-by-step plan to solve the given problem.\n",
    "        Be clear and concise in your steps.\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Problem: {question}\\n\\nProvide a step-by-step plan to solve this problem:\"),\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    plan = chain.invoke(inputs)\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "def operation_finder(inputs):\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert math solver. Identify the overall mathematical operation needed to solve the\n",
    "             problem\n",
    "        based on the given plan. Use simple operations like addition, subtraction, multiplication, and division.\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Identify the primary mathematical operation needed:\"),\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    operation = chain.invoke(inputs)\n",
    "    return {\"operation\": operation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver({\"question\": \"What is 2 + 2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## And let's say you have an eval function like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_answer(output: str, expected: dict):\n",
    "    \"\"\"The first argument is the return value from the `langchain_chain` function above.\"\"\"\n",
    "    score = int(output[\"answer\"] == expected[\"answer\"].split(\"#### \")[-1])\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Then you're probably evaluating like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "                arize_phoenix.get_dataset(name=test_set),\n",
    "                solver,\n",
    "                experiment_name=\"Experiment-Name\",\n",
    "                evaluators=[score_answer],\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Now, how can we optimize this score of 0.6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## First, initialize the Zenbase ZenbaseTracer and import the Langfuse helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.core.managers import ZenbaseTracer\n",
    "zenbase_tracer = ZenbaseTracer()\n",
    "\n",
    "from zenbase.adaptors.arize import ZenArizeAdaptor\n",
    "zen_arize_adaptor = ZenArizeAdaptor(arize_phoenix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hook up Zenbase to your functions\n",
    "\n",
    "1. Use the `zenbase_tracer` decorator.\n",
    "2. Change function inputs to request\n",
    "3. Use request's `zenbase.task_demos` to get the few-shot examples for the task and add them however you would like into your prompt.\n",
    "4. If you need to use just a few examples, you can use `request.zenbase.task_demos[:2]` to get the first two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenbase.types import LMRequest\n",
    "\n",
    "@zenbase_tracer  # it is 1\n",
    "def solver(request: LMRequest):  # it is 2\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert math solver. Solve the given problem using the provided plan and operations.\n",
    "        Return only the final numerical answer, without any additional text or explanation.\"\"\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for demo in request.zenbase.task_demos:  # it is 3\n",
    "        demo_input = demo.inputs[\"question\"]\n",
    "        demo_output = demo.outputs[\"answer\"]\n",
    "\n",
    "        messages += [\n",
    "            (\"user\", f\"Example Question: {demo_input}\"),\n",
    "            (\"assistant\", f\"Example Answer: {demo_output}\"),\n",
    "        ]  # it is 4\n",
    "\n",
    "    messages.extend(\n",
    "        [\n",
    "            (\"user\", \"Question: {question}\"),\n",
    "            (\"user\", \"Plan: {plan}\"),\n",
    "            (\"user\", \"Mathematical Operation: {operation}\"),\n",
    "            (\"user\", \"Provide the final numerical answer:\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "\n",
    "    plan = planner_chain(request.inputs)\n",
    "    operation = operation_finder({\"plan\": plan[\"plan\"], \"question\": request.inputs[\"question\"]})\n",
    "\n",
    "    inputs_to_answer = {\n",
    "        \"question\": request.inputs[\"question\"],\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"operation\": operation[\"operation\"],\n",
    "    }\n",
    "    answer = chain.invoke(inputs_to_answer)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "@zenbase_tracer  # it is 1\n",
    "def planner_chain(request: LMRequest):  # it is 2\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert math solver. Create a step-by-step plan to solve the given problem.\n",
    "        Be clear and concise in your steps.\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Problem: {question}\\n\\nProvide a step-by-step plan to solve this problem:\"),\n",
    "    ]\n",
    "\n",
    "    if request.zenbase.task_demos:  # it is 3\n",
    "        for demo in request.zenbase.task_demos[:2]:  # it is 4\n",
    "            messages += [\n",
    "                (\"user\", demo.inputs[\"question\"]),\n",
    "                (\"assistant\", demo.outputs[\"plan\"]),\n",
    "            ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    plan = chain.invoke(request.inputs)\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "@zenbase_tracer  # it is 1\n",
    "def operation_finder(request: LMRequest):  # it is 2\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert math solver. Identify the overall mathematical operation needed to solve the\n",
    "             problem\n",
    "        based on the given plan. Use simple operations like addition, subtraction, multiplication, and division.\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Identify the primary mathematical operation needed:\"),\n",
    "    ]\n",
    "\n",
    "    if request.zenbase.task_demos:  # it is 3\n",
    "        for demo in request.zenbase.task_demos[:2]:  # it is 4\n",
    "            messages += [\n",
    "                (\"user\", demo.inputs[\"question\"]),\n",
    "                (\"user\", demo.inputs[\"plan\"]),\n",
    "                (\"assistant\", demo.outputs[\"operation\"]),\n",
    "            ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    operation = chain.invoke(request.inputs)\n",
    "    return {\"operation\": operation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "return_langchain = solver({\"question\": \"What is 2 + 2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now we can optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Set up your optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.optim.metric.bootstrap_few_shot import BootstrapFewShot\n",
    "\n",
    "SHOTS = 2\n",
    "SAMPLES = 2\n",
    "\n",
    "evaluator_kwargs = dict(\n",
    "    dataset=arize_phoenix.get_dataset(name=test_set), evaluators=[score_answer]\n",
    "\n",
    ")\n",
    "\n",
    "bootstrap_few_shot = BootstrapFewShot(\n",
    "    shots=SHOTS,\n",
    "    training_set=train_set,\n",
    "    test_set=test_set,\n",
    "    validation_set=validation_set,\n",
    "    evaluator_kwargs=evaluator_kwargs,\n",
    "    zen_adaptor=zen_arize_adaptor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Do the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Empty the traces\n",
    "zenbase_tracer.all_traces = {}\n",
    "# Run the optimization\n",
    "best_fn, candidates = bootstrap_few_shot.perform(\n",
    "    solver,\n",
    "    samples=SAMPLES,\n",
    "    rounds=1,\n",
    "    trace_manager=zenbase_tracer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use your optimized function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zenbase_tracer.all_traces = {}\n",
    "best_fn({\"question\": \"What is 2+2?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Introspect function traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function_traces = [v for k, v in zenbase_tracer.all_traces.items()][0][\"optimized\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for planner_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"planner_chain\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for operation_finder chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"operation_finder\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"solver\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## How to save the function and load it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save the optimized function args to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.save_optimizer_args(\"bootstrap_few_shot_args.zenbase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the optimized function args with the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.save_optimizer_args(\"bootstrap_few_shot_args.zenbase\")\n",
    "\n",
    "optimized_function = bootstrap_few_shot.load_optimizer_and_function(\"bootstrap_few_shot_args.zenbase\", solver, zenbase_tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use the loaded function and make sure it loaded the demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zenbase_tracer.all_traces = {}\n",
    "optimized_function({\"question\": \"If I have 30% of shares, and Mo has 24.5% of shares, how many of our 10M shares are unassigned?\"})\n",
    "function_traces = [v for k, v in zenbase_tracer.all_traces.items()][0][\"optimized\"]\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"solver\"][\"args\"][\"request\"].zenbase.task_demos)\n",
    "pprint(function_traces[\"planner_chain\"][\"args\"][\"request\"].zenbase.task_demos)\n",
    "pprint(function_traces[\"operation_finder\"][\"args\"][\"request\"].zenbase.task_demos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
