{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2faf8fe22ffec95",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b07a620d54170118",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import the Zenbase Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "6bf3977f0993bbee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to install {package}: {e}\")\n",
    "        raise\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        install_package(package)\n",
    "\n",
    "try:\n",
    "    # Check if running in Google Colab\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install the zenbase package if running in Google Colab\n",
    "    # install_package('zenbase')\n",
    "    # Install the zenbse package from a GitHub branch if running in Google Colab\n",
    "    install_package('git+https://github.com/zenbase-ai/lib.git@main#egg=zenbase&subdirectory=py')\n",
    "\n",
    "    # List of other packages to install in Google Colab\n",
    "    additional_packages = [\n",
    "        'python-dotenv',\n",
    "        'langsmith[vcr]',\n",
    "        'openai',\n",
    "        'langchain',\n",
    "        'langchain_openai'\n",
    "    ]\n",
    "    \n",
    "    # Install additional packages\n",
    "    install_packages(additional_packages)\n",
    "\n",
    "# Now import the zenbase library\n",
    "try:\n",
    "    import zenbase\n",
    "except ImportError as e:\n",
    "    print(\"Failed to import zenbase: \", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445d58cc969f2c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Configure the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "18843d5c992ca261",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "#\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "load_dotenv(Path(\"../../.env.test\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "c2eb7d496006ee4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954698e9b8814e8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "f0edb99baeded13c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = wrap_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbd82f63508ffe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Now, you probably already have some LLM code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fe378f058f35d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It could use the OpenAI SDK, LangChain, or anything really. But it looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9d325b0b70bc13b1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langsmith import traceable\n",
    "from langsmith.schemas import Run, Example\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "@traceable\n",
    "def solver(inputs):\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Solve the given problem using the provided plan and operations.\n",
    "        Return only the final numerical answer, without any additional text or explanation.\"\"\"),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Mathematical Operation: {operation}\"),\n",
    "        (\"user\", \"Provide the final numerical answer:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "\n",
    "    plan = planner_chain(inputs)\n",
    "    operation = operation_finder({\"plan\": plan[\"plan\"], \"question\": inputs[\"question\"]})\n",
    "    \n",
    "    inputs_to_answer = {\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"operation\": operation[\"operation\"],\n",
    "    }\n",
    "    answer = chain.invoke(inputs_to_answer)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "@traceable\n",
    "def planner_chain(inputs):\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Create a step-by-step plan to solve the given problem.\n",
    "        Be clear and concise in your steps.\"\"\"),\n",
    "        (\"user\", \"Problem: {question}\\n\\nProvide a step-by-step plan to solve this problem:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    plan = chain.invoke(inputs)\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "@traceable\n",
    "def operation_finder(inputs):\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Identify the overall mathematical operation needed to solve the problem \n",
    "        based on the given plan. Use simple operations like addition, subtraction, multiplication, and division.\"\"\"),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Identify the primary mathematical operation needed:\")\n",
    "    ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    operation = chain.invoke(inputs)\n",
    "    return {\"operation\": operation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe027029abaaa32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## And let's say you have an eval function like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "8988857b16ac219a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_answer(run: Run, example: Example):\n",
    "    output = run.outputs[\"answer\"].split(\"#### \")[-1]\n",
    "    target = example.outputs[\"answer\"].split(\"#### \")[-1]\n",
    "    return {\n",
    "        \"key\": \"correctness\",\n",
    "        \"score\": int(output == target),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fc3e28a2caf8dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Then you're probably evaluating like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d6e862c7ed25c8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate using LangSmith\n",
    "from langsmith import Client, evaluate\n",
    "\n",
    "langsmith = Client()\n",
    "evalset = list(langsmith.list_examples(dataset_name=\"GSM8K_test_set_langsmith_dataset_2j24kEFx8T718mqwRblcoNK3S0L\"))\n",
    "\n",
    "evaluate_kwargs = dict(\n",
    "    data=evalset,\n",
    "    evaluators=[score_answer],\n",
    "    client=langsmith,\n",
    "    max_concurrency=2,\n",
    ")\n",
    "\n",
    "evaluate(solver, **evaluate_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c840156f606ea12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Now, how can we optimize this score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0648efeb26ae2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## First, initialize the Zenbase ZenbaseTracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "b83d69ca6d717355",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.core.managers import ZenbaseTracer\n",
    "zenbase_tracer = ZenbaseTracer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a8e33ff2e1476",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hook up Zenbase to your functions\n",
    "\n",
    "1. Use the `zenbase_tracer` decorator.\n",
    "2. Change function inputs to request\n",
    "3. Use request's `zenbase.task_demos` to get the few-shot examples for the task and add them however you would like into your prompt.\n",
    "4. If you need to use just a few examples, you can use `request.zenbase.task_demos[:2]` to get the first two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "e84eaff0d01b4ce9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.types import LMRequest\n",
    "\n",
    "\n",
    "@zenbase_tracer  # it is 1\n",
    "@traceable\n",
    "def solver(request: LMRequest):  # it is 2\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Solve the given problem using the provided plan and operations.\n",
    "        Return only the final numerical answer, without any additional text or explanation.\"\"\"),\n",
    "    ]\n",
    "\n",
    "    for demo in request.zenbase.task_demos: # it is 3\n",
    "        messages += [\n",
    "            (\"user\", f'Example Question: {demo.inputs[\"question\"]}'),\n",
    "            (\"assistant\", f'Example Answer: {demo.outputs[\"answer\"]}'),\n",
    "        ] # it is 4\n",
    "    \n",
    "    messages.extend([\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Mathematical Operation: {operation}\"),\n",
    "        (\"user\", \"Provide the final numerical answer:\")\n",
    "    ])\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "\n",
    "    plan = planner_chain(request.inputs)\n",
    "    operation = operation_finder({\"plan\": plan[\"plan\"], \"question\": request.inputs[\"question\"]})\n",
    "\n",
    "    inputs_to_answer = {\n",
    "        \"question\": request.inputs[\"question\"],\n",
    "        \"plan\": plan[\"plan\"],\n",
    "        \"operation\": operation[\"operation\"],\n",
    "    }\n",
    "    answer = chain.invoke(inputs_to_answer)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "@zenbase_tracer  # it is 1\n",
    "@traceable\n",
    "def planner_chain(request: LMRequest):  # it is 2\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Create a step-by-step plan to solve the given problem.\n",
    "        Be clear and concise in your steps.\"\"\"),\n",
    "        (\"user\", \"Problem: {question}\\n\\nProvide a step-by-step plan to solve this problem:\")\n",
    "    ]\n",
    "\n",
    "    if request.zenbase.task_demos:  # it is 3\n",
    "        for demo in request.zenbase.task_demos[:2]:  # it is 4\n",
    "            messages += [\n",
    "                (\"user\", demo.inputs[\"question\"]),\n",
    "                (\"assistant\", demo.outputs[\"plan\"]),\n",
    "            ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    plan = chain.invoke(request.inputs)\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "@zenbase_tracer  # it is 1\n",
    "@traceable\n",
    "def operation_finder(request: LMRequest):  # it is 2\n",
    "    messages = [\n",
    "        (\"system\", \"\"\"You are an expert math solver. Identify the overall mathematical operation needed to solve the problem \n",
    "        based on the given plan. Use simple operations like addition, subtraction, multiplication, and division.\"\"\"),\n",
    "        (\"user\", \"Question: {question}\"),\n",
    "        (\"user\", \"Plan: {plan}\"),\n",
    "        (\"user\", \"Identify the primary mathematical operation needed:\")\n",
    "    ]\n",
    "\n",
    "    if request.zenbase.task_demos:  # it is 3 \n",
    "        for demo in request.zenbase.task_demos[:2]:  # it is 4\n",
    "            messages += [\n",
    "                (\"user\", demo.inputs[\"question\"]),\n",
    "                (\"user\", demo.inputs[\"plan\"]),\n",
    "                (\"assistant\", demo.outputs[\"operation\"]),\n",
    "            ]\n",
    "\n",
    "    chain = ChatPromptTemplate.from_messages(messages) | ChatOpenAI(model=\"gpt-3.5-turbo\") | StrOutputParser()\n",
    "    operation = chain.invoke(request.inputs)\n",
    "    return {\"operation\": operation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69502658a1b13c09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Now we can optimize!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde1f982fe15e18",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Set up your optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "89f2e6c012aa313c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from zenbase.adaptors.langchain import ZenLangSmith\n",
    "from zenbase.optim.metric.bootstrap_few_shot import BootstrapFewShot\n",
    "\n",
    "# Define your Langsmith and helper\n",
    "langsmith = Client()\n",
    "zen_langsmith_adaptor = ZenLangSmith(client=langsmith)\n",
    "\n",
    "TRAIN_SET = \"GSM8K_train_set_langsmith_dataset_2j24jtX6pr5OFyi9IRlj2Pk29NX\"\n",
    "TEST_SET = \"GSM8K_test_set_langsmith_dataset_2j24kEFx8T718mqwRblcoNK3S0L\"\n",
    "VALIDATION_SET = \"GSM8K_validation_set_langsmith_dataset_2j24kCxc6WhYDe27hEcnhUEHPmE\"\n",
    "SHOTS = 2\n",
    "SAMPLES = 2\n",
    "\n",
    "train_set = zen_langsmith_adaptor.fetch_dataset(dataset_name=TRAIN_SET)\n",
    "test_set = zen_langsmith_adaptor.fetch_dataset(dataset_name=TEST_SET)\n",
    "validation_set = zen_langsmith_adaptor.fetch_dataset(dataset_name=VALIDATION_SET)\n",
    "\n",
    "evaluator_kwargs = dict(\n",
    "    evaluators=[score_answer],\n",
    "    client=langsmith,\n",
    "    max_concurrency=1,\n",
    ")\n",
    "\n",
    "bootstrap_few_shot = BootstrapFewShot(\n",
    "    shots=SHOTS,\n",
    "    training_set=train_set,\n",
    "    test_set=test_set,\n",
    "    validation_set=validation_set,\n",
    "    evaluator_kwargs=evaluator_kwargs,\n",
    "    zen_adaptor=zen_langsmith_adaptor,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18747ea23eb955",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Do the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9be9295b5388ed56",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_fn, candidates = bootstrap_few_shot.perform(\n",
    "    solver,\n",
    "    samples=SAMPLES,\n",
    "    rounds=1,\n",
    "    trace_manager=zenbase_tracer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d332e429cfcc962",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Introspect evaluation improvement\n",
    "\n",
    "You can see in this example that the best function has improved the evaluation score by 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "3f93138e1e4634fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.base_evaluation.evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "3ba94796c342ec0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.best_evaluation.evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aae58830f9eb66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use your optimized function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14bb0a5c0722ba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": "zenbase_tracer.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "dba9bf70fdba5c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now you can use your zenbase fn\n",
    "best_fn({\"question\": \"If I have 30% of shares, and Mo has 24.5% of shares, how many of our 10M shares are unassigned?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1157ddd5c5120c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Introspect function traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "46b344be33d4f626",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function_traces = [v for k, v in zenbase_tracer.all_traces.items()][0][\"optimized\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80078d6247a87952",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "924b8380edd54e76",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"solver\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3b921193b19d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for operation_finder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "69ed1c4a423daf0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"operation_finder\"][\"args\"][\"request\"].zenbase.task_demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6f15d179072c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check the optimized parameters for planner_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "28867d96f6119571",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"planner_chain\"][\"args\"][\"request\"].zenbase.task_demos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f558e74c3f53d2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## How to save the function and load it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5f4d638de419a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Save the optimized function args to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "1d53cf6d861371a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bootstrap_few_shot.save_optimizer_args(\"bootstrap_few_shot_args.zenbase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c526952c23498b5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load the optimized function args with the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "9784ae5bedb30bb7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimized_function = bootstrap_few_shot.load_optimizer_and_function(\"bootstrap_few_shot_args.zenbase\", solver, zenbase_tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61cb3f939f93a17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Use the loaded function and make sure it loaded the demos."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddf6b4fd99b1720f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "zenbase_tracer.flush()\n",
    "optimized_function({\"question\": \"If I have 30% of shares, and Mo has 24.5% of shares, how many of our 10M shares are unassigned?\"})\n",
    "function_traces = [v for k, v in zenbase_tracer.all_traces.items()][0][\"optimized\"]\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(function_traces[\"solver\"][\"args\"][\"request\"].zenbase.task_demos)\n",
    "pprint(function_traces[\"planner_chain\"][\"args\"][\"request\"].zenbase.task_demos)\n",
    "pprint(function_traces[\"operation_finder\"][\"args\"][\"request\"].zenbase.task_demos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
