{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import os\n",
    "#\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"...\"\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\n",
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\"\n",
    "\n",
    "load_dotenv(Path(\"../.env.test\"), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse()\n",
    "langfuse.auth_check()\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.decorators import observe\n",
    "\n",
    "@observe()\n",
    "def langchain_chain(inputs):\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert math solver. Your answer must be just the number with no separators, and nothing else. Follow the format of the examples.\",\n",
    "        ),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    "\n",
    "    chain = (\n",
    "        ChatPromptTemplate.from_messages(messages)\n",
    "        | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = chain.invoke(inputs)\n",
    "    return answer\n",
    "\n",
    "def score_answer(answer: str, expected_output: dict):\n",
    "    \"\"\"The first argument is the return value from the `langchain_chain` function above.\"\"\"\n",
    "    score = int(answer.split(\"#### \")[-1] == expected_output[\"answer\"].split(\"#### \")[-1])\n",
    "    langfuse.score(\n",
    "        name=\"correctness\",\n",
    "        value=score,\n",
    "        trace_id=langfuse.get_trace_id(),\n",
    "    )\n",
    "    return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score 0.4\n"
     ]
    }
   ],
   "source": [
    "evalset = langfuse.get_dataset(\"gsm8k-evalset\")\n",
    "\n",
    "scores = []\n",
    "for item in evalset.items:\n",
    "   answer = langchain_chain(item.input)\n",
    "   eval = score_answer(answer, item.expected_output)\n",
    "   scores.append(eval[\"score\"])\n",
    "\n",
    "print(\"Average score\", sum(scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenbase.types import LMRequest, LMDemo, deflm\n",
    "\n",
    "@deflm\n",
    "@observe()\n",
    "def zen_chain(request: LMRequest):\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert math solver. Your answer must be just the number with no separators, and nothing else. Follow the format of the examples.\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    for demo in request.zenbase.task_demos:\n",
    "        messages += [\n",
    "            (\"user\", demo.inputs[\"question\"]),\n",
    "            (\"assistant\", demo.outputs[\"answer\"]),\n",
    "        ]\n",
    "\n",
    "    messages.append((\"user\", \"{question}\"))\n",
    "\n",
    "    chain = (\n",
    "        ChatPromptTemplate.from_messages(messages)\n",
    "        | ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = chain.invoke(request.inputs)\n",
    "    return answer\n",
    "\n",
    "def score_answer(answer: str, demo: LMDemo, langfuse: Langfuse):\n",
    "    \"\"\"The first argument is the return value from the `zen_chain` function above.\"\"\"\n",
    "    score = int(answer.split(\"#### \")[-1] == demo.outputs[\"answer\"].split(\"#### \")[-1])\n",
    "    langfuse.score(\n",
    "        name=\"correctness\",\n",
    "        value=score,\n",
    "        trace_id=langfuse.get_trace_id(),\n",
    "    )\n",
    "    return {\"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenbase.optim.metric.labeled_few_shot import LabeledFewShot\n",
    "from zenbase.helpers.langfuse import ZenLangfuse\n",
    "\n",
    "optimizer = LabeledFewShot(\n",
    "    demoset=ZenLangfuse.dataset_demos(langfuse.get_dataset(\"gsm8k-demoset\")),\n",
    "    shots=3,\n",
    ")\n",
    "\n",
    "best_fn, candidate_results = optimizer.perform(\n",
    "    zen_chain,\n",
    "    evaluator=ZenLangfuse.metric_evaluator(\n",
    "        evalset=evalset,\n",
    "        evaluate=score_answer,\n",
    "        langfuse=langfuse,\n",
    "    ),\n",
    "    samples=4,\n",
    "    concurrency=1,\n",
    "    rounds=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = best_fn({\"question\": \"What is 2+2?\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython autoawait is `on`, and set to use `asyncio`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can even run your function asynchronously in a coroutine\n",
    "%autoawait\n",
    "\n",
    "await best_fn.coroutine({\n",
    "  \"question\": \"What is 2+2?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also save the zenbase params for re-use\n",
    "import pickle\n",
    "\n",
    "pickled_zenbase = pickle.dumps(best_fn.zenbase)\n",
    "zen_chain.zenbase = pickle.loads(pickled_zenbase)\n",
    "\n",
    "zen_chain({\"question\": \"What is 2 + 2?\"}) # uses the best few-shot demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
